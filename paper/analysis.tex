\documentclass[11pt]{article}
    \author{Calix Barrus, Laren Edwards, Dallan Olsen, Elijah Cox}
    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    \usepackage{hyperref}
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
 %   \usepackage{caption}
 %   \DeclareCaptionFormat{nocaption}{}
 %   \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Predicting Belief in Conspiracy Theories}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
        
    \hypertarget{abstract}{%
\section*{Abstract}\label{abstract}}

The purpose of this project was to find the factors that correlate most strongly with
a person believing or disbelieving in conspiracy theories, and if there
are factors related strongly enough to create an effective
regressor. To discover this, we took a dataset containing people's answers to 
questions regarding conspiracy $\text{theories}^{[\textbf{2}]}$, along with demographics and other information
on the participants. We use various regression models on this dataset to find
which traits most correlate to a belief in conspiracy theories.
\section*{Introduction} Are there some factors that correlate with a person
believing or disbelieving in conspiracy theories? Some believe that only
uneducated people, or overly educated people, have such beliefs, but do
these impressions hold any water? (At the very least, among those who
would complete an online survey?) For our project we will use online
survey data about belief in conspiracy theories to find what factors
most correlate to one believing or disbelieving in conspiracy theories.
This question has been a topic of research for several decades and has
only grown in importance over the past few years. As the COVID-19
pandemic has dragged on, more and more people have turned to QAnon and
Facebook for answers, instead of science. Thus, identifying traits that
lead to conspiracy theory belief remains an important topic. 

Previous studies focus on correlating certain traits to conspiracy theory belief.
For example, research done by Natasha Galliford and Adrian Furnham in
``Individual difference factors and beliefs in medical and political
conspiracy theories$\text{"}^{[\mathbf{2}]}$ attempted to find correlations between general
traits. They used both demographic and psychological identifiers (age,
race, sex, whether a person is introvered or extroverted, etc.) and
correlated them with belief in political and medical conspiracy
theories. Using step-wise regression, they concluded that religion and
age tended to be big factors in whether one is more accepting of
conspiracies. Another study, called, ``Looking under the tinfoil hat:
Clarifying the personological and psychopathological correlates of
conspiracy beliefs`` (Bowes et al., 2020), attempted to find correlation
between extremely specific traits such as abnormal personality and
psychological disorders and conspiracy theory belief. The authors used
meta-regression to determine the strength of this correlation, and found
that any personality trait that was correlated was usually weakly so.

Our project examines both general identifiers (such as religion,
race, and sex) and specific personality traits (such as is a person
reserved, anxious, disorganized) as features for regression. We use linear and Random Forest regressors on our dataset to see which factors have a
strong correlation to conspiracy theory belief. The dataset we use is
taken from''Measuring belief in conspiracy theories: the generic
conspiracist beliefs scale'', a study that used the generic
conspiracist beliefs scale (GCBS) score to quantify one's belief in
conspiracy theories. The study also asked questions about demographic
and personality traits, which we use to construct a design matrix. We
use this dataset because it has an extensive amount of data, which we
need to get a good estimator, and because all the data are available
to the public. However, it is important to keep in mind that the data
come from an internet survey. Thus, it is likely that not every person
took it seriously, and some people likely lied. To account for this, we
analyze the data to find people who may not have taken the survey
seriously (more specifically, those who took little to no time to complete the survey, put the same
answer for every question, or entered invalid responses). 

To determine which features of our data contribute to general belief in conspiracy theories, we use three forms of regression. The first is an $L_1$-regularized regression (Lasso regression), our reasoning being that our design matrix has over 100 features and we do not wish to overfit. The other (and better) methods are ElasticNet and Random Forest regressors. Each of these is discussed in more detail later. 
\section*{Dataset}
Below is an illustration of some of the questions participants in the survey were asked about their beliefs:\\
\begin{center}
\includegraphics[scale=1.1]{"../survey.png"}\\
\end{center}
The following table contains detailed descriptions of each feature of the dataset.
\newpage
    \begin{tabular}{ c|c }
        Label & Description \\
        \hline
        \textbf{TIPI2} & Question on whether the subject consider themselves \\ & critical and quarrelsome. \\
        \hline
        \textbf{TIPI5} & Question on whether the subject considers themselves \\ & open to new experiences and complex. \\
        \hline
        \textbf{TIPI6} & Question on whether the subject considers themselves \\ & reserved and quiet. \\
        \hline
        \textbf{vocabulary\_misclassification} & All subjects were given a list of 16 words and asked if \\ & they knew what each word meant. 3 of \\ & the words were not real words. So this variable computes \\ & the misclassification rate of only these 3 words to \\ & find people who say they know the definition of a fake word. \\ & This helps us know which people are more \\ & likely to inflate their knowledge. \\
        \hline
        \textbf{STEM} & This is a binary variable saying whether or not \\ & the subject majored in STEM in college. \\
        \hline
        \textbf{education\_2} & Binary variable saying whether the subject only has a \\ & high school education or some other level of education. \\
        \hline
        \textbf{education\_3}  & Binary variable saying whether the subject only has \\ & a University degree or some other level of education. \\
        \hline
        \textbf{urban\_3} & The type of area that the subjects lived in as a child. \\ & This variable is a binary variable showing they either grew \\ & up in an urban environment or not. \\
        \hline
        \textbf{gender\_2} & Binary variable saying whether a subject is female or not. \\
        \hline
        \textbf{engnat\_1} & Binary variable saying whether a subject speaks \\ & English as their native language or not. \\
        \hline
        \textbf{religion\_2} & Binary variable saying whether a subject is \\ & Atheist or not. \\
        \hline
        \textbf{religion\_3} & Binary variable saying whether a subject is \\ & Buddhist or not. \\
        \hline
        \textbf{religion\_7} & Binary variable saying whether a subject is a \\ & non-Catholic, non-LDS, non-Protestant Christian or not. \\
        \hline
        \textbf{religion\_12} & Binary variable saying whether a subject is part \\ & of a religion that is not any major religion. \\
        \hline
        \textbf{orientation\_2} & Binary variable saying whether a subject's \\ & sexual orientation is bisexual or not. \\
        \hline
        \textbf{orientation\_5} & Binary variable saying whether a subject's sexual \\ & orientation is outside of heterosexual, homosexual, \\ & bisexual, and asexual. \\
        \hline
        \textbf{voted\_2} & Binary variable saying whether the subject didn't \\ & vote in the last national election. \\
        \hline
        \textbf{married\_1} & Binary variable saying whether the subject has \\ & never been married. \\
    \end{tabular}
%\newpage
\section*{Data Cleaning and Feature Engineering} The
survey includes many important features including (but not limited to)
age, education, religion, major, and vocabulary comprehension. Most were
categorized and one-hot encoded, but not all. Here we go over the
changes we make to fields with text or categorical values. 
\subsubsection*{GCB - General Conspiracy Belief}
The first field we create is the GCB field, or ``General Conspiracy
Belief''. The first 15 questions of the survey asked about level of
belief in specific conspiracies on a 1-5 scale. The questions were in
the form of statements, like the following example: ``Groups of
scientists manipulate, fabricate, or suppress evidence in order to
deceive the public.'' The GCB is the average of these values for each
person, as computed in the following code block, so 0.5 would indicate an average level of belief of 0.5 (or a response of 2.5 in the survey, which is on a scale from 1 to 5) in
various conspiracies.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[}\StringTok{\textquotesingle{}GCB\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df[[}\StringTok{\textquotesingle{}Q\textquotesingle{}}\OperatorTok{+}\BuiltInTok{str}\NormalTok{(i) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in} \BuiltInTok{range}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{16}\NormalTok{)]].mean(axis}\OperatorTok{=}\DecValTok{1}\NormalTok{) }\OperatorTok{/} \DecValTok{5}
\end{Highlighting}
\end{Shaded}

This is used as the target ``response'' variable in our analysis. It
simplifies the regression a lot to use an average of these answers
rather than trying to have 15 different response variables.

\hypertarget{major}{%
\subsubsection*{Major}\label{major}}

We make several changes to the [college] ``major'' field.
Here, survey respondents put in their major as a string; because
this was a free response, we have hundreds of unique responses.
Because there are so many majors (and so many major misspellings), we
group the responses together in larger categories, namely
STEM, Humanities, Business/Law, Arts, and Other.
We understand that we are making the simplifying assumption that majors within the same
sector to have similar correlations to a person's belief in
conspiracies, but this loss in information is worth the time it saves us, as we no longer have to inspect each response. Because there were such varied responses, we initially
assigned each major to a group manually. We then one-hot encoded these
new variables to facilitate performance of classification. But in order to
enable future expansion of the data used, we needed to implement a flexible
major classification strategy that could sort new major strings robustly
into the existing categories. We do this using Levenshtein edit
distance, which is
calculated using recursion to determine the
difference between two strings in terms of how many primitive edits it
takes to permute one into the other. The major category with the lowest Levenshtein distance to any newly observed response is taken to be the correct category for that response. Although this may result in some misclassified majors, using a natural language processing method to improve performance is outside the scope of this paper.

Below is the code detecting unassigned majors and using the Levenshtein
function to categorize the majors accordingly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unassigned }\OperatorTok{=}\NormalTok{ df[df[}\StringTok{"STEM"}\NormalTok{]}\OperatorTok{+}\NormalTok{df[}\StringTok{"HUM"}\NormalTok{]}\OperatorTok{+}\NormalTok{df[}\StringTok{"BUS"}\NormalTok{]}\OperatorTok{+}\NormalTok{df[}\StringTok{"OTHER"}\NormalTok{]}\OperatorTok{+}\NormalTok{df[}\StringTok{"ART"}\NormalTok{] }\OperatorTok{==} \DecValTok{0}\NormalTok{]}
\NormalTok{categories }\OperatorTok{=}\NormalTok{ [}\StringTok{"STEM"}\NormalTok{, }\StringTok{"HUM"}\NormalTok{, }\StringTok{"BUS"}\NormalTok{, }\StringTok{"OTHER"}\NormalTok{, }\StringTok{"ART"}\NormalTok{]}
\NormalTok{score }\OperatorTok{=} \BuiltInTok{dict}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ index,unknown\_string }\KeywordTok{in} \BuiltInTok{zip}\NormalTok{(unassigned.index,unassigned[}\StringTok{\textquotesingle{}major\textquotesingle{}}\NormalTok{]):}
    \ControlFlowTok{for}\NormalTok{ category }\KeywordTok{in}\NormalTok{ categories:}
\NormalTok{        tf }\OperatorTok{=} \BuiltInTok{open}\NormalTok{(}\SpecialStringTok{f"}\SpecialCharTok{\{}\NormalTok{category}\SpecialCharTok{\}}\SpecialStringTok{.txt"}\NormalTok{, }\StringTok{"r"}\NormalTok{,newline}\OperatorTok{=}\StringTok{\textquotesingle{}}\CharTok{\textbackslash{}n}\StringTok{\textquotesingle{}}\NormalTok{)}
\NormalTok{        majors }\OperatorTok{=}\NormalTok{ [i[:}\OperatorTok{{-}}\DecValTok{2}\NormalTok{] }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ tf.readlines()]}
\NormalTok{        score[category] }\OperatorTok{=} \BuiltInTok{min}\NormalTok{([levenshteinDistance(}\BuiltInTok{str}\NormalTok{(maj),}\BuiltInTok{str}\NormalTok{(unknown\_string))}\OperatorTok{\textbackslash{}}
\ControlFlowTok{	for}\NormalTok{ maj }\KeywordTok{in}\NormalTok{ majors])}
\NormalTok{    min\_key }\OperatorTok{=} \BuiltInTok{min}\NormalTok{(score, key}\OperatorTok{=}\NormalTok{score.get)}
\NormalTok{    df.loc[index,min\_key] }\OperatorTok{=} \DecValTok{1}
\end{Highlighting}
\end{Shaded}

\hypertarget{vocabulary_knowledge-vocabulary_misclassification}{%
\subsubsection*{Vocabulary Knowledge and
Vocabulary Misclassification}\label{vocabulary_knowledge-vocabulary_misclassification}}

The survey we use includes questions in which the respondent identified
the meaning of various vocabulary words. Correct answers for each
question were recorded as 1s, and incorrect as 0s. We perform a simple
averaging calculation to determine a person's approximate vocabulary
knowledge (conceivably this works as a very rough proxy for general
lexicographic knowledge). Omitted VCL\_\_ questions are categorized
into a separate variable "validity", since they were nonsense words
included to prevent overconfident respondents from getting a perfect score.
As illustrated in the code below, vocabulary questions 6, 9, and 12 were the
nonsense words, and the rest of the questions are a true vocabulary knowledge
test.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{df[}\StringTok{\textquotesingle{}validity\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df[[}\StringTok{\textquotesingle{}VCL6\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}VCL9\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}VCL12\textquotesingle{}}\NormalTok{]].mean(axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
knowledge \OperatorTok{=} [1,2,3,4,5,7,8,10,11,13,14,15,16]
\NormalTok{df[}\StringTok{\textquotesingle{}vocabulary\_knowledge\textquotesingle{}}\NormalTok{] }\OperatorTok{=}\NormalTok{ df[[}\StringTok{\textquotesingle{}VCL\textquotesingle{}} \OperatorTok{+} \BuiltInTok{str}\NormalTok{(i) }\ControlFlowTok{for}\NormalTok{ i }\KeywordTok{in}\NormalTok{ knowledge]].mean(axis}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{time-feature-engineered}{%
\subsubsection*{Response Time}\label{time-feature-engineered}}

For further feature engineering, we examine the respondents who took
the least amount of time and the most time to complete the survey, as
well as the respondents who put the same answer for every question on
belief in conspiracy theories. However, upon visual inspection of all of
these edge cases, the demographic information seemed appropriately
varied, and was completely filled out, implying that these rows should
not be thrown out. We concluded that it is likely that the publishers of
this survey data performed similar data cleaning and so any unfinished
surveys or invalid responses would already be thrown out.

%\newpage
\hypertarget{methods}{%
\section*{Methods}\label{methods}}

As mentioned in the introduction, the three methods we compare are $L_1$-regularized least-squares regression, ElasticNet regression, and a Random Forest ensemble of regression trees. We first use least-squares regression with \(L_1\)-regularization, hoping to remove any irrelevant features.
We use $\alpha=1\times10^{-3}$, which reduces
the 100+ columns down to 20 columns with nonzero
coefficients. We then run OLS without any regularization on the reduced set
of columns simply so that the results table won't include features with coefficients of zero (and because, at the time of writing, statsmodels has not yet implemented the summary display for regularized regression models). The next methods we use are ElasticNet and Random Forest regression. In both
cases we use a grid search across various hyperparameters, illustrated in Table~\ref{tab:hyper} in the Results section, with 5-way cross validation, looking to
maximize the classifier's \(R^2\) value. With the Random Forests, we
limited the possible tree parameters to preclude overfitting. In fact,
by letting the max tree depth be 10, we found the tree to have an
\(R^2\) value of over .5, much higher than the \(.15\)-\(.2 R^2\) values
found by the least-squares estimators. We concluded that the forest was likely
very overfit, and so restricted ourselves to more conservative hyper
parameters. Each Random Forest we tried used between 100 and 200 tree estimators, with the optimal Random Forest using 200 trees.
\newpage
\section*{Results}
 Table~\ref{tab:lasso}
shows the coefficient of determination \(R^2\) and correlation
coefficients for different features for our initial attempt using \(L_1\)-regularization.
\begin{table}[ht]
	\centering
	\caption{Lasso Regression Results.} \label{tab:lasso}
	\vspace{1mm}
\begin{tabular}{lclc}
\toprule
\textbf{Dep. Variable:}                &       GCB        & \textbf{  R-squared:         } &     0.135   \\
\textbf{Model:}                        &       OLS        & \textbf{  Adj. R-squared:    } &     0.129   \\
\textbf{Method:}                       &  Least Squares   & \textbf{  F-statistic:       } &     24.15   \\
\textbf{Date:}                         & Wed, 01 Dec 2021 & \textbf{  Prob (F-statistic):} &  8.15e-67   \\
\textbf{Time:}                         &     17:11:36     & \textbf{  Log-Likelihood:    } &    573.41   \\
\textbf{No. Observations:}             &        2495      & \textbf{  AIC:               } &    -1113.   \\
\textbf{Df Residuals:}                 &        2478      & \textbf{  BIC:               } &    -1014.   \\
\textbf{Df Model:}                     &          16      & \textbf{                     } &             \\
\textbf{Covariance Type:}              &    nonrobust     & \textbf{                     } &             \\
\bottomrule
\end{tabular}
\begin{tabular}{lcccccc}
                                       & \textbf{coef} & \textbf{std err} & \textbf{t} & \textbf{P$> |$t$|$} & \textbf{[0.025} & \textbf{0.975]}  \\
\midrule
\textbf{TIPI1}                         &       0.0046  &        0.003     &     1.817  &         0.069        &       -0.000    &        0.010     \\
\textbf{TIPI2}                         &       0.0130  &        0.002     &     5.968  &         0.000        &        0.009    &        0.017     \\
\textbf{TIPI5}                         &       0.0038  &        0.003     &     1.321  &         0.187        &       -0.002    &        0.009     \\
\textbf{TIPI6}                         &       0.0003  &        0.003     &     0.101  &         0.919        &       -0.005    &        0.005     \\
\textbf{TIPI7}                         &       0.0013  &        0.002     &     0.541  &         0.589        &       -0.003    &        0.006     \\
\textbf{vocabulary\_misclassification} &       0.0707  &        0.017     &     4.045  &         0.000        &        0.036    &        0.105     \\
\textbf{OTHER}                         &       0.0228  &        0.008     &     2.699  &         0.007        &        0.006    &        0.039     \\
\textbf{education\_2}                  &       0.0301  &        0.008     &     3.637  &         0.000        &        0.014    &        0.046     \\
\textbf{urban\_3}                      &       0.0237  &        0.008     &     2.890  &         0.004        &        0.008    &        0.040     \\
\textbf{gender\_2}                     &       0.0229  &        0.008     &     2.880  &         0.004        &        0.007    &        0.039     \\
\textbf{religion\_2}                   &      -0.0756  &        0.009     &    -8.253  &         0.000        &       -0.094    &       -0.058     \\
\textbf{religion\_3}                   &       0.0864  &        0.028     &     3.092  &         0.002        &        0.032    &        0.141     \\
\textbf{religion\_7}                   &       0.0639  &        0.014     &     4.449  &         0.000        &        0.036    &        0.092     \\
\textbf{religion\_12}                  &       0.1034  &        0.013     &     8.256  &         0.000        &        0.079    &        0.128     \\
\textbf{orientation\_5}                &       0.0438  &        0.017     &     2.591  &         0.010        &        0.011    &        0.077     \\
\textbf{voted\_2}                      &       0.0208  &        0.009     &     2.434  &         0.015        &        0.004    &        0.038     \\
\textbf{const}                         &       0.4135  &        0.028     &    14.870  &         0.000        &        0.359    &        0.468     \\
\bottomrule
\end{tabular}
\begin{tabular}{lclc}
\textbf{Omnibus:}       & 76.163 & \textbf{  Durbin-Watson:     } &    1.910  \\
\textbf{Prob(Omnibus):} &  0.000 & \textbf{  Jarque-Bera (JB):  } &   40.646  \\
\textbf{Skew:}          &  0.123 & \textbf{  Prob(JB):          } & 1.49e-09  \\
\textbf{Kurtosis:}      &  2.425 & \textbf{  Cond. No.          } &     80.3  \\
\bottomrule
\end{tabular}
\end{table}

None of our features seems to be strongly correlated with GCB, and we
only account for about 13.5\% of the variance in beliefs with these
features. Among what we do find, however, the strongest predictors of
belief in conspiracy theories seem to be religion (in particular marking
`other' as one's choice of religion), age, and low knowledge of the given
vocabulary words, as determined by the engineered features ``vocabulary
knowledge'' and ``vocabulary\_misclassification''.

We achieve better results using a grid search on the scikit-learn implementations
of ElasticNet and RandomForestRegressor. As Table~\ref{tab:hyper} illustrates, we achieve the best results (\(R^2\) =
.2297) using a Random Forest. Note that the Random
Forest listed here is not the best (in terms of \(R^2\)) Random Forest
that we were able to generate, but it is the best that has hyper
parameters that don't lend themselves to overfitting. 

\begin{table}[ht]
	\centering
	\caption{Best Estimators and Results.} \label{tab:hyper} 
	\vspace{1mm}
\begin{tabular}{ |c|c|c| } 
       \hline
       Model & Best Hyperparameters & $R^2$ \\ 
       \hline
       sklearn ElasticNet & $\alpha = .001$, \verb|l1_ratio| $= .4842$ & .1811 \\ 
       sklearn RandomForestRegressor & \verb|max_depth| = 5, \verb|min_samples_leaf| = 6 & .2297 \\
       \hline
	\end{tabular}
\end{table}

In Table~\ref{tab:gini}, we see which features are most important in the
Random Forest. Note that a high Gini importance indicates that a feature
is important in determining whether or not a person believes in
conspiracy theories, but that feature is not necessarily positively
correlated (it could be negatively correlated). For example, the way the least-squares
regression interpreted these features, religion\_2,
vocabulary\_knowledge, and TIPI10 were negatively correlated with GCB, and
religion\_12 and age were positively correlated with GCB.

\begin{table}[ht]
	\centering
	\caption{Feature Importance.} \label{tab:gini}
	\vspace{1mm}
\begin{tabular}{ |c|c|c| } 
   \hline
   Feature & Meaning & Gini Importance \\ 
   \hline
   \verb|religion_2| & Atheist & .2040 \\ 
   \verb|vocabulary_knowledge| & \% of words marked correctly & .1749 \\ 
   \verb|religion_12| & "Other" Religion & .1218 \\ 
   \verb|age| & Years Old & .0664 \\ 
   \verb|TIPI10| & Conventional, Uncreative Personality & .0566 \\ 
   \hline
\end{tabular}
\end{table}

These results indicate that among the group of people that was
surveyed, there are a number of features which are definitely correlated
with belief in conspiracy theories. It is possible
that these features are also correlated with conspiracy
belief in the population in general. However, it is important to note
that 
\begin{enumerate}
\item our \(R^2\) values indicate that our model accounts for very
little of the variance in belief in conspiracy beliefs and 
\item since this was
an online volunteer survey, we have no reason to believe that the
respondents are representative of society or any general group.
\end{enumerate}

With these things in mind, we note that the strongest predictors of belief in conspiracy theories seem to be religion (in particular marking ‘other’ or 'atheist' as one’s choice of religion), low knowledge of the given vocabulary words, and age. The two most important quantitative variables are the engineered vocabulary score features and age; Figure \ref{fig:heat} is a heatmap of General Conspiracy Belief in relation to a person’s age and vocabulary score. Although we do see a concentration of higher GCB in the lower age/lower vocab score area, there is no strong correlation immediately apparent. This matches up with the low predictive power of our various regressors - correlations do exist, but are not strong.
\begin{figure}[ht]
	\centering
\includegraphics[scale=1.2]{"../VocabAge_GCB.png"}
	\vspace{-14mm}
	\caption{Heatmap of the most Gini-important quantitative features.}
	\label{fig:heat}
\end{figure}
%\newpage
    \hypertarget{ethical-implications}{%
\section*{Ethical Implications}\label{ethical-implications}}

It is important to remember that, although we have found traits with
positive and negative correlation to conspiracy theory belief, our model
has a low $R^2$ score, so it does not account
for a lot of the variance in the data. Thus, just because someone may
have many of the traits that correlate with conspiracy theory belief,
that does not mean they do believe in conspiracy theories. Human beings
are diverse and multi-faceted in their beliefs. Thus, absolute
assumptions should not be made from the data above. Instead, the data
above should be used as merely a soft guide to indicate that a person
may believe conspiracy theories. This could be useful in anticipating a
group's beliefs prior to giving them information about controversial
topics, or in determining the issues most important to a group of
people. But it should not be relied on for any important decisions.

Additionally, it is important that predictions from this model not be
used to discriminate against individuals or disqualify them from
anything, such as jobs or political offices. High correlations are good
leads for other questions of research, such as ``why is belief in
conspiracy highly correlated with \_\_\_\_?'' Understanding the causes
behind the trends could help us answer questions about nature
vs.~nurture, psychology, and other topics in the field of study.

\newpage
\hypertarget{references}{%
\section*{References}\label{references}}
\begin{enumerate}
\item Bowes, S. M., Costello, T. H., Ma, W., \& Lilienfeld, S. O. (2021).
Looking under the tinfoil hat: Clarifying the personological and
psychopathological correlates of conspiracy beliefs. Journal of
Personality, 89(3), 422-436.
\newline\url{https://doi.org/https://doi.org/10.1111/jopy.12588}

\item Galliford, N., \& Furnham, A. (2017). Individual difference factors and beliefs in medical
and political conspiracy theories. Scandinavian Journal of Psychology,
58(5), 422-428. \newline\url{https://doi.org/https://doi.org/10.1111/sjop.12382}

\item Brotherton, R., French, C. C., \& Pickering, A. D. (2013). Measuring belief in conspiracy theories: The generic conspiracist beliefs scale. Frontiers in Psychology, 4, 279. \newline\url{https://doi.org/10.3389/fpsyg.2013.00279}

\end{enumerate}


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
