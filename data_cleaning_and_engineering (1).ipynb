{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "import statsmodels.regression.linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data and Engineer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = \"data/\"\n",
    "df = pd.read_csv(data_directory + \"conspiracy_theories_data_orig.csv\")\n",
    "verbose = False\n",
    "# Only NaN values are in \"major\" column, so no other cleaning is necessary to remove NaN values\n",
    "# Benefit of working with survey data as opposed to data collected using messier methods\n",
    "\n",
    "# Measure for General Conspiracy Belief. Normalized average of responses to questions 1-15 of survey\n",
    "df['GCB'] = df[['Q'+str(i) for i in range(1, 16)]].mean(axis=1) / 5\n",
    "\n",
    "# Score how many vocab questions the respondent answered correctly. 0 is correct for VCL 6, 9, 12, and 1 is correct for all others.\n",
    "df['vocabulary_knowledge'] = pd.concat((df[['VCL' + str(i) for i in [1, 2, 3, 4, 5, 7, 8, 10, 11, 13, 14, 15, 16]]], \n",
    "                                        (1 - df[['VCL' + str(i) for i in [6,9,12]]])), axis=1).mean(axis=1)\n",
    "\n",
    "# The survey asked participants what words they knew. Columns VCL6, VCL9, VCL12 were not real words, and were included in \n",
    "# order to perform a validity check\n",
    "df['vocabulary_misclassification'] = df[['VCL6', 'VCL9', 'VCL12']].mean(axis=1)\n",
    "\n",
    "# Split up every instance of \"major\" to a category: HUM (Humanities), BUS (business/law), ART, STEM, and OTHER. \n",
    "# This block creates a one-hot encoding for each of these.\n",
    "names = [\"STEM\", \"HUM\", \"BUS\", \"OTHER\", \"ART\"]\n",
    "for name in names:\n",
    "    # For each category, there is a file of strings of majors that should be classified as that category\n",
    "    # Read in the corresponding file\n",
    "    tf = open(data_directory + f\"{name}.txt\", \"r\",newline='\\n')\n",
    "    # Grab all the strings in the file\n",
    "    majors = [i[:-2] for i in tf.readlines()]\n",
    "    def func(x): # If string is in the list of majors, return a 1, else a 0\n",
    "        return int(x in majors)\n",
    "    func = np.vectorize(func)\n",
    "    df[name] = 1 \n",
    "    df[name] = df.major.apply(func) # Create  a new column with the one hot encoding for the given category\n",
    "    \n",
    "# One hot encode the other categorical features\n",
    "categorical_columns = ['education','urban', 'gender', 'engnat', 'hand', 'religion', 'orientation','race', 'voted', 'married']\n",
    "df = pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
    "df[\"constant\"] = 1\n",
    "\n",
    "# Engineer some features about the time taken on the survey for reviewing edge cases (not to be used in regression).\n",
    "df[\"total_time_taken_(mins)\"] = (df[\"introelapse\"] + df[\"testelapse\"] + df[\"surveyelapse\"])/60\n",
    "df[\"total_survey_time_taken_(mins)\"] = (df[\"testelapse\"] + df[\"surveyelapse\"])/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Edge Cases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Surveys that took over an hour to take (including landing pad time)\n",
      "64\n",
      "# Surveys that took over an hour to take (excluding landing pad time)\n",
      "20\n",
      "# Surveys that spent over an hour on the landing pad\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "print(\"# Surveys that took over an hour to take (including landing pad time)\")\n",
    "print(sum(df[\"total_time_taken_(mins)\"] >= 60))\n",
    "\n",
    "print(\"# Surveys that took over an hour to take (excluding landing pad time)\")\n",
    "print(sum(df[\"total_survey_time_taken_(mins)\"] >= 60))\n",
    "\n",
    "print(\"# Surveys that spent over an hour on the landing pad\")\n",
    "print(sum(df[\"introelapse\"]/60 >= 60))\n",
    "\n",
    "if verbose: \n",
    "    df[\"total_time_taken_(mins)\"][df[\"total_time_taken_(mins)\"] < 60].hist()\n",
    "    plt.subplots()\n",
    "    df[\"total_survey_time_taken_(mins)\"][df[\"total_survey_time_taken_(mins)\"] < 60].hist()\n",
    "    plt.subplots()\n",
    "    df[\"introelapse\"][df[\"introelapse\"] < 60].hist()\n",
    "\n",
    "# Even though these surveys took a lot longer than seems reasonable, there are no clear indications in the \n",
    "# subjects' answers that any of these responses should be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose: \n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "        display(df[df[\"introelapse\"]/60 >= 60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the 50 fastest responses\n",
    "if verbose:\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        display(df.sort_values(by=\"total_time_taken_(mins)\")[:50])\n",
    "    \n",
    "# Again, none of these look responses have any obvious indications that they should be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Rows with matching entries in columsn Q1, Q2, ..., Q15\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "# Did any respondents put the same thing for each question in the GCB inventory? \n",
    "print(\"# Rows with matching entries in columsn Q1, Q2, ..., Q15\")\n",
    "print(sum(df[[\"Q\" + str(i) for i in range(1, 16)]].apply(lambda x: min(x) == max(x), 1)))\n",
    "if verbose:\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        display(df[df[[\"Q\" + str(i) for i in range(1, 16)]].apply(lambda x: min(x) == max(x), 1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all the columns that are used for feature engineering and cleaning.\n",
    "df.drop(columns=['Q'+str(i) for i in range(1, 16)], inplace=True) # Drop the specific question information from which GCB is computed\n",
    "df.drop(columns=['E'+str(i) for i in range(1, 16)], inplace=True) # Timing information\n",
    "df.drop(columns=['VCL'+str(i) for i in range(1, 17)], inplace=True) # Specific vocabulary questions, rolled into \n",
    "df.drop(columns=['total_time_taken_(mins)', 'total_survey_time_taken_(mins)', \n",
    "                 'introelapse', 'surveyelapse', 'testelapse'], inplace=True)\n",
    "df.drop(columns=['major'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['GCB']\n",
    "X = df.drop(columns=['GCB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    GCB   R-squared:                       0.186\n",
      "Model:                            OLS   Adj. R-squared:                  0.165\n",
      "Method:                 Least Squares   F-statistic:                     9.087\n",
      "Date:                Tue, 30 Nov 2021   Prob (F-statistic):           3.33e-71\n",
      "Time:                        19:06:19   Log-Likelihood:                 648.68\n",
      "No. Observations:                2495   AIC:                            -1173.\n",
      "Df Residuals:                    2433   BIC:                            -812.4\n",
      "Df Model:                          61                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================================\n",
      "                                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------------\n",
      "TIPI1                            0.0039      0.003      1.495      0.135      -0.001       0.009\n",
      "TIPI2                            0.0120      0.002      5.217      0.000       0.007       0.017\n",
      "TIPI3                            0.0022      0.003      0.837      0.402      -0.003       0.007\n",
      "TIPI4                            0.0006      0.003      0.235      0.814      -0.004       0.006\n",
      "TIPI5                            0.0034      0.003      1.113      0.266      -0.003       0.009\n",
      "TIPI6                            0.0029      0.003      1.057      0.291      -0.002       0.008\n",
      "TIPI7                            0.0002      0.002      0.066      0.948      -0.005       0.005\n",
      "TIPI8                            0.0008      0.002      0.363      0.716      -0.004       0.005\n",
      "TIPI9                           -0.0046      0.003     -1.712      0.087      -0.010       0.001\n",
      "TIPI10                          -0.0105      0.002     -4.300      0.000      -0.015      -0.006\n",
      "age                           1.131e-05    5.6e-06      2.019      0.044    3.26e-07    2.23e-05\n",
      "familysize                       0.0026      0.002      1.440      0.150      -0.001       0.006\n",
      "vocabulary_knowledge            -0.2482      0.032     -7.855      0.000      -0.310      -0.186\n",
      "vocabulary_misclassification     0.0393      0.018      2.187      0.029       0.004       0.075\n",
      "STEM                            -0.0406      0.013     -3.206      0.001      -0.065      -0.016\n",
      "HUM                             -0.0191      0.012     -1.545      0.123      -0.043       0.005\n",
      "BUS                             -0.0077      0.021     -0.364      0.716      -0.049       0.034\n",
      "OTHER                           -0.0144      0.022     -0.659      0.510      -0.057       0.028\n",
      "ART                              0.0167      0.021      0.790      0.430      -0.025       0.058\n",
      "education_1                     -0.0221      0.033     -0.666      0.506      -0.087       0.043\n",
      "education_2                      0.0194      0.032      0.606      0.545      -0.043       0.082\n",
      "education_3                      0.0071      0.033      0.218      0.827      -0.057       0.071\n",
      "education_4                     -0.0101      0.034     -0.298      0.766      -0.076       0.056\n",
      "urban_1                         -0.0043      0.033     -0.128      0.898      -0.070       0.061\n",
      "urban_2                         -0.0167      0.033     -0.513      0.608      -0.081       0.047\n",
      "urban_3                          0.0128      0.033      0.390      0.697      -0.051       0.077\n",
      "gender_1                        -0.0177      0.078     -0.225      0.822      -0.172       0.136\n",
      "gender_2                         0.0023      0.078      0.029      0.976      -0.152       0.156\n",
      "gender_3                         0.0008      0.081      0.010      0.992      -0.157       0.159\n",
      "engnat_1                        -0.0267      0.069     -0.388      0.698      -0.161       0.108\n",
      "engnat_2                        -0.0483      0.069     -0.700      0.484      -0.184       0.087\n",
      "hand_1                           0.0121      0.056      0.215      0.830      -0.098       0.122\n",
      "hand_2                           0.0146      0.057      0.255      0.799      -0.098       0.127\n",
      "hand_3                           0.0277      0.059      0.470      0.638      -0.088       0.143\n",
      "religion_1                       0.0057      0.037      0.153      0.878      -0.067       0.078\n",
      "religion_2                      -0.0624      0.037     -1.688      0.092      -0.135       0.010\n",
      "religion_3                       0.0788      0.045      1.739      0.082      -0.010       0.168\n",
      "religion_4                       0.0058      0.038      0.152      0.879      -0.070       0.081\n",
      "religion_5                       0.0176      0.050      0.349      0.727      -0.081       0.116\n",
      "religion_6                      -0.0075      0.039     -0.193      0.847      -0.084       0.069\n",
      "religion_7                       0.0535      0.038      1.396      0.163      -0.022       0.129\n",
      "religion_8                       0.0360      0.054      0.667      0.505      -0.070       0.142\n",
      "religion_9                      -0.0889      0.054     -1.633      0.103      -0.196       0.018\n",
      "religion_10                      0.0466      0.048      0.978      0.328      -0.047       0.140\n",
      "religion_11                     -0.0855      0.101     -0.844      0.399      -0.284       0.113\n",
      "religion_12                      0.1021      0.038      2.707      0.007       0.028       0.176\n",
      "orientation_1                    0.0023      0.035      0.067      0.946      -0.066       0.071\n",
      "orientation_2                   -0.0126      0.036     -0.351      0.726      -0.083       0.058\n",
      "orientation_3                    0.0044      0.038      0.116      0.908      -0.070       0.079\n",
      "orientation_4                   -0.0156      0.038     -0.407      0.684      -0.091       0.060\n",
      "orientation_5                    0.0312      0.038      0.819      0.413      -0.043       0.106\n",
      "race_1                           0.0100      0.046      0.218      0.827      -0.080       0.100\n",
      "race_2                           0.0114      0.060      0.190      0.850      -0.106       0.129\n",
      "race_3                           0.0644      0.052      1.246      0.213      -0.037       0.166\n",
      "race_4                          -0.0015      0.043     -0.035      0.972      -0.086       0.083\n",
      "race_5                           0.0110      0.045      0.248      0.804      -0.076       0.098\n",
      "voted_1                         -0.0563      0.043     -1.313      0.189      -0.141       0.028\n",
      "voted_2                         -0.0361      0.043     -0.848      0.397      -0.120       0.047\n",
      "married_1                       -0.0114      0.052     -0.217      0.828      -0.114       0.091\n",
      "married_2                        0.0273      0.053      0.515      0.607      -0.077       0.131\n",
      "married_3                        0.0374      0.055      0.685      0.493      -0.070       0.144\n",
      "constant                         0.7750      0.135      5.739      0.000       0.510       1.040\n",
      "==============================================================================\n",
      "Omnibus:                       56.784   Durbin-Watson:                   1.942\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               32.340\n",
      "Skew:                           0.099   Prob(JB):                     9.49e-08\n",
      "Kurtosis:                       2.479   Cond. No.                     3.17e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.17e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Statsmodels\n",
    "###### Run a L1-regularization to see which features we should keep and what we can get rid of #######\n",
    "\n",
    "# lamb_list = np.geomspace(10**-10, 10**5, 16)\n",
    "\n",
    "# # # Go through each lambda\n",
    "# #for lamb in lamb_list:\n",
    "# model = lm.OLS(y, X).fit_regularized(alpha=1e-3, L1_wt=1)\n",
    "# beta = model.params\n",
    "# nonzero = np.abs(beta) > 0.01\n",
    "# zero = ~nonzero\n",
    "# # X = X[X.columns[zero.values]]\n",
    "# X = X[X.columns[nonzero.values]]\n",
    "\n",
    "model = lm.OLS(y, X).fit()\n",
    "# print(model.summary().as_latex())\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1811793008060938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.001, 'l1_ratio': 0.4842105263157895}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimize R^2 with regularization\n",
    "y = df['GCB']\n",
    "X = df.drop(columns=['GCB'])\n",
    "# Elasticnet Regularization with Gridsearch (L1, L2)\n",
    "parameters = {\n",
    "              'alpha': [10**(-k) for k in range(1,6)],\n",
    "              'l1_ratio': np.linspace(.4,.6,20)\n",
    "}\n",
    "\n",
    "reg = ElasticNet()\n",
    "clf = GridSearchCV(reg, parameters, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "print(clf.score(X,y))\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.15397058780752398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.001, 'l1_ratio': 0.4736842105263158}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimize MSE with regularization\n",
    "y = df['GCB']\n",
    "X = df.drop(columns=['GCB'])\n",
    "# Elasticnet Regularization with Gridsearch (L1, L2)\n",
    "parameters = {\n",
    "              'alpha': [10**(-k) for k in range(1,6)],\n",
    "              'l1_ratio': np.linspace(0.4,0.6,20)\n",
    "}\n",
    "\n",
    "reg = ElasticNet()\n",
    "clf = GridSearchCV(reg, parameters, n_jobs=-1, scoring='neg_mean_absolute_error')\n",
    "clf.fit(X, y)\n",
    "print(clf.score(X,y))\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5481237203766918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'min_samples_leaf': 4, 'n_estimators': 200}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Regressor with Gridsearch\n",
    "parameters = {\n",
    "    'n_estimators': [100,200],\n",
    "    'max_depth': list(range(1,6)) + [10],\n",
    "    'min_samples_leaf': range(1,6),\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "clf = GridSearchCV(rf, parameters, n_jobs=-1)\n",
    "clf.fit(X,y)\n",
    "print(clf.score(X,y))\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2351911874302588\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'min_samples_leaf': 1, 'n_estimators': 200}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Regressor with Gridsearch\n",
    "parameters = {\n",
    "    'n_estimators': [100,200],\n",
    "    'max_depth': list(range(1,6)),\n",
    "    'min_samples_leaf': range(1,6),\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "clf = GridSearchCV(rf, parameters, n_jobs=-1)\n",
    "clf.fit(X,y)\n",
    "print(clf.score(X,y))\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22966611679185056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5, 'min_samples_leaf': 6, 'n_estimators': 200}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest Regressor with Gridsearch\n",
    "parameters = {\n",
    "    'n_estimators': [100,200],\n",
    "    'max_depth': list(range(1,6)),\n",
    "    'min_samples_leaf': range(4,7),\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "clf = GridSearchCV(rf, parameters, n_jobs=-1)\n",
    "clf.fit(X,y)\n",
    "print(clf.score(X,y))\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 0.0, 9.124107731619417e-05, 0.00012207194480844181,\n",
       "        0.0001938673986796815, 0.00025974329712405593,\n",
       "        0.0006927415545757902, 0.0007263298863353116,\n",
       "        0.000815923138509695, 0.0008275030822420464,\n",
       "        0.0008578810663197829, 0.0010830252001160382,\n",
       "        0.0011083442833396727, 0.0011449829465474152,\n",
       "        0.001145842255362119, 0.001540254471214687,\n",
       "        0.0016071223364955002, 0.0016409387201648579,\n",
       "        0.0017077784406807617, 0.0017519202838357528,\n",
       "        0.0019662450130379875, 0.0020010237507443658,\n",
       "        0.002034717041107469, 0.002211368376556814, 0.00230174724328229,\n",
       "        0.0025509011208090697, 0.0027558947920163905,\n",
       "        0.0028990085074772176, 0.002959561946296944,\n",
       "        0.0031354891316252324, 0.003446092388660418,\n",
       "        0.0037294454683868103, 0.003784787676644177,\n",
       "        0.004282470192074333, 0.00485118331626192, 0.004898386961778732,\n",
       "        0.005507590071058269, 0.005951093090431952, 0.006678925740819176,\n",
       "        0.006715391169527864, 0.006724233573208364, 0.009323609108951617,\n",
       "        0.009537794376370182, 0.009639221648009214, 0.009706884769683535,\n",
       "        0.011141161975409408, 0.012218697725816971, 0.014365104182696306,\n",
       "        0.014642419509851966, 0.014787013060780682, 0.0159992992760863,\n",
       "        0.017471486628051237, 0.018275658364591454, 0.020331441179967852,\n",
       "        0.02328803978778972, 0.030764123320092027, 0.04593694320092192,\n",
       "        0.056628467423475735, 0.06642521067157864, 0.12184277178078072,\n",
       "        0.17493065540541103, 0.20404092864821013],\n",
       "       ['constant', 'religion_11', 'religion_5', 'religion_8',\n",
       "        'religion_4', 'race_2', 'hand_2', 'race_1', 'race_3',\n",
       "        'orientation_4', 'BUS', 'education_1', 'religion_3', 'gender_3',\n",
       "        'religion_10', 'orientation_3', 'religion_1', 'engnat_2',\n",
       "        'orientation_1', 'orientation_2', 'religion_6', 'hand_1', 'HUM',\n",
       "        'voted_2', 'engnat_1', 'married_1', 'race_5', 'OTHER',\n",
       "        'married_2', 'gender_2', 'religion_9', 'race_4', 'education_3',\n",
       "        'urban_2', 'married_3', 'ART', 'urban_1', 'gender_1',\n",
       "        'education_4', 'urban_3', 'orientation_5', 'hand_3', 'TIPI8',\n",
       "        'education_2', 'religion_7', 'voted_1', 'TIPI6', 'TIPI7',\n",
       "        'TIPI3', 'vocabulary_misclassification', 'STEM', 'TIPI4',\n",
       "        'TIPI9', 'TIPI5', 'TIPI1', 'familysize', 'TIPI2', 'TIPI10',\n",
       "        'age', 'religion_12', 'vocabulary_knowledge', 'religion_2']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = clf.best_estimator_.feature_importances_\n",
    "order = np.argsort(importances)\n",
    "np.vstack((importances[order], X.columns.values[order]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13934509783847715"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elasticnet Regularization with Gridsearch (L1, L2)\n",
    "parameters = {\n",
    "              'alpha': [10**(-k) for k in range(1,6)],\n",
    "              'l1_ratio': [k/10 for k in range(1,10)]\n",
    "#               'l1_ratio': [0,1]\n",
    "}\n",
    "\n",
    "reg = ElasticNet()\n",
    "clf = GridSearchCV(reg, parameters, n_jobs=-1)\n",
    "clf.fit(X, y)\n",
    "clf.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
